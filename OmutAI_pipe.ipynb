{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym==0.26.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "NWGT_AXjXeXm",
        "outputId": "36db1989-71e4-4bee-834b-433aa80832f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/721.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/721.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/721.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827628 sha256=faf7ff23cd0dd13a0e6a13fcfd38939a289e3243f67269c3ce3e759eb9c00648\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "8b38475f431340a6b0b609dfb8b4c4e7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bx6QyLaXV6vo"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch.distributions\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "\n",
        "from collections import  deque\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, in_dim: int = 4, hidden_dim: int = 128, out_dim: int = 2):\n",
        "        super(Policy, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=in_dim, out_features=hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_out = nn.Linear(in_features=hidden_dim, out_features=out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x_out = self.fc_out(x)\n",
        "        return F.softmax(x_out, dim=-1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action), m.entropy()\n",
        "\n",
        "\n",
        "class ValueFunction(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        super(ValueFunction, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=input_dim, out_features=2*hidden_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.fc_out = nn.Linear(in_features=2*hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x_out = self.fc_out(x)\n",
        "        return x_out\n",
        "\n"
      ],
      "metadata": {
        "id": "6tP7DaViWAzR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_episodes: int, policy, env):\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    policy.train()\n",
        "    value_function.train()\n",
        "    gamma = 0.99\n",
        "    alpha = 0.08\n",
        "    seeds = [42, 1234, 555, 52]\n",
        "    baseline = 0.0\n",
        "    for episode in range(num_episodes):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "        state = env.reset(seed=seeds[episode % len(seeds)])[0]\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, log_prob, entropy = policy.act(state)\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            entropies.append(entropy)\n",
        "\n",
        "            if reward > 1900:\n",
        "                break\n",
        "\n",
        "        n = len(rewards)\n",
        "        G = 0\n",
        "        discounted_rewards = deque()\n",
        "        for t in reversed(range(n)):\n",
        "            G = rewards[t] + gamma * G\n",
        "            discounted_rewards.appendleft(G)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
        "\n",
        "        entropies = torch.tensor(entropies).float()\n",
        "\n",
        "        policy_loss = []\n",
        "        baseline = torch.mean(torch.tensor(discounted_rewards).float())\n",
        "        # baseline = 0.25 * baseline + 0.75 * torch.mean(torch.tensor(discounted_rewards).float())\n",
        "        for discount_reward, log_prob, entropy in zip(discounted_rewards, log_probs, entropies):\n",
        "            policy_loss.append(-(discount_reward - baseline + alpha * entropy) * log_prob)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Episode: {episode}, loss: {policy_loss}, reward: {np.sum(rewards)}')\n",
        "        print('===========')\n",
        "\n"
      ],
      "metadata": {
        "id": "zoLLo4g_WBdr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_value_function(num_episodes: int, policy, value_function, env):\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-2, weight_decay=1e-3)\n",
        "    optimizer_value_function = torch.optim.Adam(value_function.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    mse_loss_value = nn.MSELoss()\n",
        "    policy.train()\n",
        "    value_function.train()\n",
        "    gamma = 0.99\n",
        "    alpha = 0.1\n",
        "    for episode in range(num_episodes):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        state_values = []\n",
        "        prev_state_values = []\n",
        "        states = []\n",
        "        entropies = []\n",
        "        state = env.reset(seed=42)[0] # add different seeds\n",
        "        done = False\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            action, log_prob, entropy = policy.act(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            states.append(state)\n",
        "            entropies.append(entropy)\n",
        "\n",
        "            if len(state_values) > 0:\n",
        "                prev_state_values.append(state_values[-1])\n",
        "            state_values.append(value_function(state_tensor))\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "        n = len(rewards)\n",
        "        G = 0\n",
        "        discounted_rewards = deque()\n",
        "        for t in reversed(range(n)):\n",
        "            G = rewards[t] + gamma * G\n",
        "            discounted_rewards.appendleft(G)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
        "\n",
        "        entropies = torch.tensor(entropies).float()\n",
        "\n",
        "        policy_loss = []\n",
        "        for discount_reward, log_prob, state_value, prev_state_value, entropy in zip(discounted_rewards, log_probs, state_values, prev_state_values, entropies):\n",
        "            cur_state_value = state_value.detach().clone()\n",
        "            cur_prev_state_value = prev_state_value.detach().clone()\n",
        "            cur_state_value.requires_grad = False\n",
        "            cur_prev_state_value.requires_grad = False\n",
        "            policy_loss.append(-(discount_reward + cur_state_value - cur_prev_state_value + alpha * entropy) * log_prob)\n",
        "\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        value_loss = mse_loss_value(torch.cat(state_values).squeeze(), discounted_rewards)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        optimizer_value_function.zero_grad()\n",
        "        value_loss.backward()\n",
        "        optimizer_value_function.step()\n",
        "        print(f'Episode: {episode}, loss: {policy_loss}, value_loss: {value_loss.item()}, reward: {np.sum(rewards)}')\n",
        "        print('===========')\n"
      ],
      "metadata": {
        "id": "n9sK4hz-WBfu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_rloo(num_episodes: int, policy, env):\n",
        "    def get_rloo(rewards):\n",
        "        n = len(rewards)\n",
        "        rloo_values = []\n",
        "        for i in range(n):\n",
        "            rloo_value = (sum(rewards) - rewards[i]) / (n - 1)\n",
        "            rloo_values.append(rloo_value / n)\n",
        "        return rloo_values\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[300, 420], gamma=0.9)\n",
        "    policy.train()\n",
        "    value_function.train()\n",
        "    gamma = 0.99\n",
        "    alpha = 0.08\n",
        "    seeds = [42, 1234, 555, 52]\n",
        "    for episode in range(num_episodes):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "        state = env.reset(seed=seeds[episode % len(seeds)])[0]\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, log_prob, entropy = policy.act(state)\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            entropies.append(entropy)\n",
        "\n",
        "            if sum(rewards) > 1900:\n",
        "                break\n",
        "\n",
        "        n = len(rewards)\n",
        "        G = 0\n",
        "        discounted_rewards = deque()\n",
        "        for t in reversed(range(n)):\n",
        "            G = rewards[t] + gamma * G\n",
        "            discounted_rewards.appendleft(G)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
        "\n",
        "        entropies = torch.tensor(entropies).float()\n",
        "\n",
        "        policy_loss = []\n",
        "        rloo_values = torch.tensor(get_rloo(discounted_rewards))\n",
        "        for discounted_reward, rloo_value, log_prob, entropy in zip(discounted_rewards, rloo_values, log_probs, entropies):\n",
        "            policy_loss.append(-(discounted_reward - rloo_value + alpha * entropy) * log_prob)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        print(f'Episode: {episode}, loss: {policy_loss}, reward: {np.sum(rewards)}')\n",
        "        print('===========')\n",
        "\n"
      ],
      "metadata": {
        "id": "W7SmusmMWBik"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(policy, env):\n",
        "    policy.eval()\n",
        "    seeds = [42, 1234, 555, 52]\n",
        "    for seed in seeds:\n",
        "        total_reward = 0.0\n",
        "        state = env.reset(seed=seed)[0]\n",
        "        done = False\n",
        "        while not done and total_reward < 1900:\n",
        "            action, log_prob, entropy = policy.act(state)\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "        print(f'Seed: {seed}, total_reward: {total_reward}')\n"
      ],
      "metadata": {
        "id": "nbHBUc2oWMgv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset(seed=42)\n",
        "\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "policy = Policy(in_dim=input_dim, hidden_dim=128, out_dim=output_dim)\n",
        "value_function = ValueFunction(input_dim=input_dim, hidden_dim=16, output_dim=1)\n",
        "train(num_episodes=500, policy=policy, env=env)\n",
        "\n",
        "train_with_value_function(num_episodes=500, policy=policy, value_function=value_function, env=env)\n",
        "\n",
        "train_with_rloo(num_episodes=500, policy=policy, env=env)\n",
        "\n",
        "eval(policy, env)\n",
        "\n"
      ],
      "metadata": {
        "id": "fcWtiUvyWNVc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEQMZSD8WSY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5F51eSlnWTR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}